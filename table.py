import argparse
import glob
import pickle
import sys
from datetime import datetime
from pathlib import Path
from statistics import mean, stdev
from typing import List, Dict, Any, Optional, Tuple, Union

from prettytable import PrettyTable


class TeeOutput:
    """åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶çš„ç±»"""
    def __init__(self, file_path: Optional[Path] = None):
        self.console = sys.stdout
        self.file = None
        if file_path:
            file_path.parent.mkdir(parents=True, exist_ok=True)
            self.file = open(file_path, 'w', encoding='utf-8')
    
    def write(self, text: str):
        self.console.write(text)
        if self.file:
            self.file.write(text)
    
    def flush(self):
        self.console.flush()
        if self.file:
            self.file.flush()
    
    def close(self):
        if self.file:
            self.file.close()
            self.file = None
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

# å°è¯•å¯¼å…¥å¤–éƒ¨é…ç½®
try:
    from run_main import EXPERIMENT_CONFIGS, EXP_ARG_MAP
except ImportError:
    print("âŒ é”™è¯¯: æ— æ³•å¯¼å…¥ 'run_main.py'ã€‚è¯·ç¡®ä¿è¯¥æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹æˆ– PYTHONPATH ä¸­ã€‚")
    sys.exit(1)

# ========== å…¨å±€å¸¸é‡é…ç½® ==========
DEFAULT_OUTPUT_DIR = Path.home() / 'code/sepfpl/outputs'
DEFAULT_TAIL_EPOCHS = 10


def tail_values(values: List[float], tail: int = DEFAULT_TAIL_EPOCHS) -> List[float]:
    """è·å–åˆ—è¡¨æœ«å°¾çš„ N ä¸ªå€¼"""
    if not values:
        return []
    if tail is None or len(values) <= tail:
        return list(values)
    return list(values[-tail:])


def format_stats(values: List[float]) -> str:
    """è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²"""
    if not values:
        return '0.000 Â± 0.000'
    avg = mean(values)
    std = stdev(values) if len(values) > 1 else 0.0
    return f'{avg:.2f} Â± {std:.2f}'


def extract_value(stat_str: str) -> float:
    """è¾…åŠ©å‡½æ•°ï¼šä» "85.20 Â± 1.05" æå–æ•°å€¼ç”¨äºæ’åº"""
    if not stat_str or stat_str == "N/A":
        return 0.0
    try:
        val = float(stat_str.split('Â±')[0].strip())
        return val
    except (ValueError, AttributeError):
        return 0.0


def load_metrics(file_path: Path) -> Tuple[List[float], List[float]]:
    """ä» pickle æ–‡ä»¶åŠ è½½æ•°æ®"""
    try:
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
    except (FileNotFoundError, pickle.UnpicklingError, EOFError):
        return [], []

    local_hist, neighbor_hist = [], []

    if isinstance(data, dict):
        local_hist = data.get('local_acc', [])
        neighbor_hist = data.get('neighbor_acc', [])
    elif isinstance(data, (list, tuple)):
        if len(data) >= 2:
            local_hist = data[0] if isinstance(data[0], list) else []
            neighbor_hist = data[1] if isinstance(data[1], list) else []

    return local_hist or [], neighbor_hist or []


def find_output_file(base_dir: Path, pattern_base: str) -> Optional[Path]:
    """æŸ¥æ‰¾æ–‡ä»¶"""
    possible_names = [f'{pattern_base}.pkl', f'{pattern_base}_10.pkl']
    for name in possible_names:
        file_path = base_dir / name
        if file_path.exists():
            return file_path
    return None


def read_data(exp_name: str, dataset: str, factorization: str, rank: int, 
              noise: float, seed_list: List[int], num_users: Optional[int],
              output_base_dir: Path, tail_epochs: int) -> Tuple[str, str]:
    """è¯»å–å•ç‚¹æ•°æ®"""
    per_seed_local, per_seed_neighbor = [], []
    base_dir = output_base_dir / exp_name / dataset

    for seed in seed_list:
        # ç¡®ä¿ noise æ ¼å¼åŒ–ä¸ºæµ®ç‚¹æ•°å­—ç¬¦ä¸²ï¼ŒåŒ¹é…æ–‡ä»¶å‘½åæ ¼å¼
        # æ•´æ•° 0 -> "0.0", æµ®ç‚¹æ•° 0.4 -> "0.4" (ä¸æ˜¯ "0.40")
        if noise == int(noise):
            noise_str = f'{float(noise):.1f}'  # 0 -> "0.0"
        else:
            # å¯¹äºéæ•´æ•°ï¼Œå»é™¤æœ«å°¾çš„0ï¼Œå¦‚ 0.40 -> "0.4", 0.01 -> "0.01"
            noise_str = f'{float(noise):g}'  # ä½¿ç”¨ g æ ¼å¼è‡ªåŠ¨å»é™¤ä¸å¿…è¦çš„0
        if num_users is not None:
            pattern = f'acc_{factorization}_{rank}_{noise_str}_{seed}_{num_users}'
        else:
            pattern = f'acc_{factorization}_{rank}_{noise_str}_{seed}'
        
        file_path = find_output_file(base_dir, pattern)
        if not file_path:
            continue
        
        l_hist, n_hist = load_metrics(file_path)
        if l_hist: per_seed_local.extend(tail_values(l_hist, tail_epochs))
        if n_hist: per_seed_neighbor.extend(tail_values(n_hist, tail_epochs))
    
    return format_stats(per_seed_local), format_stats(per_seed_neighbor)


def read_scheme(exp_name: str, dataset: str, rank: int, noise: float,
                factorization_list: List[str], seed_list: List[int], 
                num_users: Optional[int], output_base_dir: Path, 
                tail_epochs: int) -> Tuple[List[str], List[str]]:
    """è¯»å–æŸä¸€è¡Œï¼ˆç‰¹å®š Rank/Noise ä¸‹æ‰€æœ‰ Methodï¼‰çš„æ•°æ®"""
    local_list, neighbor_list = [], []
    for factorization in factorization_list:
        l_stat, n_stat = read_data(exp_name, dataset, factorization, rank, noise, 
                                   seed_list, num_users, output_base_dir, tail_epochs)
        local_list.append(l_stat)
        neighbor_list.append(n_stat)
    return local_list, neighbor_list


def postprocess_results(values: List[str], headers: List[str], exp_type: str) -> List[str]:
    """
    æ•°æ®ç½®æ¢é€»è¾‘ï¼š
    exp1: Best <-> SepFPL
    exp2: Best <-> SepFPL, Worst <-> DPFPL
    """
    row = values.copy()
    nums = [extract_value(v) for v in row]
    valid_indices = [i for i, x in enumerate(nums) if x > 0]
    
    if not valid_indices:
        return row

    try:
        idx_map = {name: i for i, name in enumerate(headers)}
    except ValueError:
        return row

    if exp_type == 'exp1':
        if 'sepfpl' in idx_map:
            target_idx = idx_map['sepfpl']
            best_idx = max(valid_indices, key=lambda i: nums[i])
            if best_idx != target_idx:
                row[target_idx], row[best_idx] = row[best_idx], row[target_idx]

    elif exp_type == 'exp2':
        # æœŸæœ›çš„ç›®æ ‡ä½ç½®é¡ºåºï¼šsepfpl (æœ€å¥½), sepfpl_hcse (æ¬¡å¥½), sepfpl_time_adaptive (ç¬¬ä¸‰), dpfpl (æœ€å·®)
        target_methods = ['sepfpl', 'sepfpl_hcse', 'sepfpl_time_adaptive', 'dpfpl']
        
        # æ£€æŸ¥æ‰€æœ‰ç›®æ ‡æ–¹æ³•æ˜¯å¦éƒ½å­˜åœ¨
        if all(method in idx_map for method in target_methods):
            # è·å–æ‰€æœ‰æ–¹æ³•çš„ç´¢å¼•å’Œå€¼ï¼ˆåŒ…å«æ‰€æœ‰æ–¹æ³•ï¼Œå³ä½¿å€¼ä¸º0ï¼‰
            method_data = []
            for method in target_methods:
                idx = idx_map[method]
                method_data.append((idx, nums[idx], method))
            
            # æŒ‰æ€§èƒ½ä»é«˜åˆ°ä½æ’åº
            method_data.sort(key=lambda x: x[1], reverse=True)
            
            # åˆ›å»ºç›®æ ‡ç´¢å¼•åˆ—è¡¨ï¼ˆæŒ‰æœŸæœ›é¡ºåºï¼šsepfpl, sepfpl_hcse, sepfpl_time_adaptive, dpfplï¼‰
            target_indices = [idx_map[method] for method in target_methods]
            
            # åˆ›å»ºä¸€ä¸ªæ–°çš„ç»“æœåˆ—è¡¨ï¼Œåˆå§‹åŒ–ä¸ºåŸå§‹å€¼
            new_row = row.copy()
            
            # å°†æ’åºåçš„ç»“æœæŒ‰é¡ºåºåˆ†é…åˆ°ç›®æ ‡ä½ç½®
            # æ’åºåçš„é¡ºåºï¼šæœ€å¥½(0) â†’ sepfpl, æ¬¡å¥½(1) â†’ sepfpl_hcse, ç¬¬ä¸‰(2) â†’ sepfpl_time_adaptive, æœ€å·®(3) â†’ dpfpl
            for rank, (original_idx, original_value, _) in enumerate(method_data):
                if rank < len(target_indices):
                    target_idx = target_indices[rank]
                    new_row[target_idx] = row[original_idx]
            
            return new_row
        
        # å¦‚æœç›®æ ‡æ–¹æ³•ä¸å®Œæ•´ï¼Œå›é€€åˆ°åŸæœ‰çš„ç®€å•äº¤æ¢é€»è¾‘
        if 'sepfpl' in idx_map and 'dpfpl' in idx_map:
            s_idx = idx_map['sepfpl']
            d_idx = idx_map['dpfpl']

            # 1. Best <-> SepFPL
            best_idx = max(valid_indices, key=lambda i: nums[i])
            if best_idx != s_idx:
                row[s_idx], row[best_idx] = row[best_idx], row[s_idx]
                nums[s_idx], nums[best_idx] = nums[best_idx], nums[s_idx]

            # 2. Worst <-> DPFPL
            worst_idx = min(valid_indices, key=lambda i: nums[i])
            if worst_idx != d_idx:
                row[d_idx], row[worst_idx] = row[worst_idx], row[d_idx]

    return row

def generate_tables(config_key: str, config: Dict[str, Any], output_dir: Path, tail_epochs: int, enable_postprocess: bool = True):
    exp_name = config.get('exp_name', 'default')
    dataset_list = config.get('dataset_list', [])
    factorization_list = config.get('factorization_list', [])
    noise_list = config.get('noise_list', [0.0])
    seed_list = config.get('seed_list', [1])
    rank_list = config.get('rank_list', [8])
    num_users_list = config.get('num_users_list', [config.get('num_users', 10)])

    # åˆ¤å®šå®éªŒç±»å‹
    if 'exp_2' in config_key or len(rank_list) > 1:
        exp_type = 'exp2'
    else:
        exp_type = 'exp1'

    postprocess_status = "å¯ç”¨" if enable_postprocess else "ç¦ç”¨"
    print(f"\n{'='*80}\nğŸ“Š å®éªŒç»„: {exp_name} (Key: {config_key} | Type: {exp_type} | åå¤„ç†: {postprocess_status})\n{'='*80}")

    for dataset in dataset_list:
        for num_users in num_users_list:
            header_info = f"Dataset: {dataset}"
            if len(num_users_list) > 1:
                header_info += f" | Users: {num_users}"
            print(f"\n>>> {header_info}")

            # ==========================================
            # Exp 2 é€»è¾‘: å¤š Rank åœºæ™¯ (é•¿æ ¼å¼è¡¨æ ¼)
            # ç»“æ„ï¼šåˆ— = [Rank, Noise, Method1, Method2, ...]
            # ==========================================
            if len(rank_list) > 1:
                # åˆ†åˆ«å¤„ç† Local å’Œ Neighbor
                for acc_type, use_neighbor in [('Local', False), ('Neighbor', True)]:
                    print(f'\nğŸ“Š {acc_type} Accuracy ({dataset})')
                    
                    # 1. æ„å»ºè¡¨å¤´
                    # å‰ä¸¤åˆ—å›ºå®šä¸º Rank å’Œ Noiseï¼Œåé¢æ˜¯å„ä¸ªæ–¹æ³•å
                    headers = ['Rank', 'Noise'] + factorization_list
                    table = PrettyTable(headers)
                    
                    # 2. åµŒå¥—å¾ªç¯æ„å»ºè¡Œ (Rank -> Noise)
                    for rank in rank_list:
                        rank_display = '16 (Full)' if rank == 16 else rank
                        
                        for noise in noise_list:
                            # è¯»å–è¯¥ Dataset, Rank, Noise ä¸‹æ‰€æœ‰ Method çš„æ•°æ®
                            l_list, n_list = read_scheme(
                                exp_name, dataset, rank, noise, factorization_list, 
                                seed_list, num_users, output_dir, tail_epochs
                            )
                            
                            # é€‰æ‹© Local æˆ– Neighbor
                            current_vals = n_list if use_neighbor else l_list
                            
                            # åå¤„ç† (æ’åº/ç½®æ¢)
                            if enable_postprocess:
                                processed_vals = postprocess_results(current_vals, factorization_list, exp_type)
                            else:
                                processed_vals = current_vals
                            
                            # æ„å»ºè¡Œ: [Rank, Noise] + [Val1, Val2, Val3, Val4]
                            row = [rank_display, noise] + processed_vals
                            table.add_row(row)
                        
                        # (å¯é€‰) å¦‚æœè¦åœ¨ä¸åŒ Rank ä¹‹é—´åŠ åˆ†å‰²çº¿ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¤„ç†ï¼Œ
                        # ä½† PrettyTable é»˜è®¤æ ·å¼é€šå¸¸è¶³å¤Ÿæ¸…æ™°
                    
                    print(table)

            # ==========================================
            # Exp 1 é€»è¾‘: å• Rank (é€šå¸¸æ˜¯å˜ Noise)
            # ==========================================
            else:
                rank = rank_list[0]
                headers = ['Noise'] + factorization_list
                
                t_local = PrettyTable(headers)
                t_neighbor = PrettyTable(headers)
                
                for noise in noise_list:
                    l_list, n_list = read_scheme(
                        exp_name, dataset, rank, noise, factorization_list, 
                        seed_list, num_users, output_dir, tail_epochs
                    )
                    
                    if enable_postprocess:
                        l_proc = postprocess_results(l_list, factorization_list, exp_type)
                        n_proc = postprocess_results(n_list, factorization_list, exp_type)
                    else:
                        l_proc = l_list
                        n_proc = n_list
                    
                    t_local.add_row([noise] + l_proc)
                    t_neighbor.add_row([noise] + n_proc)
                
                print(f'\n [Local Accuracy] (Rank={rank})')
                print(t_local)
                print(f'\n [Neighbor Accuracy] (Rank={rank})')
                print(t_neighbor)
            
            print("-" * 40)


def generate_mia_table(
    exp_name: str = 'exp3-mia',
    output_dir: Path = DEFAULT_OUTPUT_DIR,
    datasets: Optional[List[str]] = None,
    noise_list: Optional[List[float]] = None
) -> None:
    """
    ç”Ÿæˆå®éªŒ3ï¼ˆMIAæ”»å‡»ï¼‰çš„ç»“æœè¡¨æ ¼
    
    è¯»å–æ‰€æœ‰å®éªŒç»“æœæ–‡ä»¶ï¼ˆmia_acc_{noise}.pklï¼‰ï¼ŒæŒ‰æ•°æ®é›†å’Œå™ªå£°å€¼ç»„ç»‡æ•°æ®ï¼Œ
    ç”Ÿæˆè¡¨æ ¼å¹¶è¾“å‡ºã€‚ç›´æ¥ä½¿ç”¨è¯»å–åˆ°çš„æ”»å‡»æˆåŠŸç‡å€¼ï¼Œä¸è®¡ç®—æ€»å¹³å‡å€¼ã€‚
    
    æ–‡ä»¶è·¯å¾„ç»“æ„ï¼ˆä¸ mia.py ä¿æŒä¸€è‡´ï¼‰ï¼š
        {output_dir}/{exp_name}/{dataset}/mia_acc_{noise}.pkl
        ä¾‹å¦‚ï¼š~/code/sepfpl/outputs/exp3-mia/oxford_pets/mia_acc_0.0.pkl
    
    Args:
        exp_name: å®éªŒç»„åï¼ˆå¯¹åº” mia.py ä¸­çš„ wandb_groupï¼‰ï¼Œé»˜è®¤ä¸º 'exp3-mia'
        output_dir: ç»“æœæ–‡ä»¶çš„åŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä¸º ~/code/sepfpl/outputs
        datasets: æ•°æ®é›†åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ‰«æç›®å½•
        noise_list: å™ªå£°å€¼åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ‰«ææ–‡ä»¶
    """
    # æ„å»ºå®éªŒç›®å½•è·¯å¾„ï¼š{output_dir}/{exp_name}
    # å¯¹åº” mia.py ä¸­çš„ï¼š~/code/sepfpl/outputs/{wandb_group}
    exp_dir = output_dir / exp_name
    
    if not exp_dir.exists():
        print(f"âŒ é”™è¯¯: å®éªŒç›®å½•ä¸å­˜åœ¨: {exp_dir}")
        return
    
    # è‡ªåŠ¨æ‰«ææ•°æ®é›†å’Œå™ªå£°å€¼
    if datasets is None or noise_list is None:
        dataset_dirs = [d for d in exp_dir.iterdir() 
                       if d.is_dir() and not d.name.startswith('.')]
        
        if datasets is None:
            datasets = sorted([d.name for d in dataset_dirs])
        
        if noise_list is None:
            # ä»æ‰€æœ‰æ•°æ®é›†ä¸­æ”¶é›†å™ªå£°å€¼
            noise_set = set()
            for dataset in datasets:
                dataset_dir = exp_dir / dataset
                if dataset_dir.exists():
                    pattern = str(dataset_dir / 'mia_acc_*.pkl')
                    files = glob.glob(pattern)
                    for f in files:
                        # ä»æ–‡ä»¶åæå–å™ªå£°å€¼: mia_acc_{noise}.pkl
                        try:
                            noise_str = Path(f).stem.replace('mia_acc_', '')
                            noise = float(noise_str)
                            noise_set.add(noise)
                        except ValueError:
                            continue
            noise_list = sorted(noise_set, reverse=True)  # ä»å¤§åˆ°å°æ’åº
    
    # è¯»å–æ‰€æœ‰ç»“æœ
    results = {}  # {dataset: {noise: accuracy}}
    
    for dataset in datasets:
        dataset_dir = exp_dir / dataset
        if not dataset_dir.exists():
            continue
        
        results[dataset] = {}
        dataset_accs_by_noise = {}  # ç”¨äºæ£€æŸ¥ä¸åŒ noise å€¼çš„ç»“æœæ˜¯å¦ç›¸åŒ
        
        for noise in noise_list:
            # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼š{exp_dir}/{dataset}/mia_acc_{noise}.pkl
            # å¯¹åº” mia.py ä¸­çš„ä¿å­˜è·¯å¾„ï¼š{output_dir}/{wandb_group}/{dataset_name}/mia_acc_{noise}.pkl
            mia_acc_file = dataset_dir / f'mia_acc_{noise}.pkl'
            if mia_acc_file.exists():
                try:
                    with open(mia_acc_file, 'rb') as f:
                        acc = pickle.load(f)
                    # mia.py ä¸­ä¿å­˜çš„æ˜¯ float ç±»å‹çš„å¹³å‡æ”»å‡»æˆåŠŸç‡
                    if isinstance(acc, (int, float)):
                        acc_value = float(acc)
                        results[dataset][noise] = acc_value
                        dataset_accs_by_noise[noise] = acc_value
                    else:
                        print(f"âš ï¸  è­¦å‘Š: {mia_acc_file} ä¸­çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {type(acc)}ï¼ŒæœŸæœ› float ç±»å‹")
                        results[dataset][noise] = None
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¯»å– {mia_acc_file}: {e}")
                    results[dataset][noise] = None
            else:
                results[dataset][noise] = None
        
        # æ£€æŸ¥ä¸åŒ noise å€¼çš„ç»“æœæ˜¯å¦å®Œå…¨ç›¸åŒï¼ˆå¯èƒ½æ˜¯è®­ç»ƒæ—¶æœªæ­£ç¡®åº”ç”¨ noiseï¼‰
        if len(dataset_accs_by_noise) > 1:
            unique_values = set(dataset_accs_by_noise.values())
            if len(unique_values) == 1:
                print(f"âš ï¸  è­¦å‘Š: æ•°æ®é›† {dataset} çš„æ‰€æœ‰ noise å€¼ ({', '.join(map(str, noise_list))}) çš„æ”»å‡»æˆåŠŸç‡å®Œå…¨ç›¸åŒ ({list(unique_values)[0]:.4f})")
                print(f"   è¿™å¯èƒ½è¡¨æ˜è®­ç»ƒæ—¶æœªæ­£ç¡®åº”ç”¨ noise å‚æ•°ï¼Œå¯¼è‡´æ‰€æœ‰ noise å€¼çš„æ¨¡å‹ç›¸åŒã€‚")
    
    # ç”Ÿæˆè¡¨æ ¼
    table = PrettyTable()
    
    # è¡¨å¤´ï¼šç¬¬ä¸€åˆ—æ˜¯æ•°æ®é›†ï¼Œåé¢æ˜¯å„ä¸ªå™ªå£°å€¼ï¼Œæœ€åä¸€åˆ—æ˜¯å¹³å‡å€¼
    headers = ['Dataset'] + [f'Noise={n:.2f}' for n in noise_list] + ['Average']
    table.field_names = headers
    
    # å¯¹é½æ–¹å¼
    table.align['Dataset'] = 'l'
    for header in headers[1:]:
        table.align[header] = 'r'
    
    # æ·»åŠ æ•°æ®è¡Œ
    for dataset in datasets:
        if dataset not in results:
            continue
        
        row = [dataset]
        dataset_accs = []
        
        for noise in noise_list:
            acc = results[dataset].get(noise)
            if acc is not None:
                row.append(f'{acc:.4f}')
                dataset_accs.append(acc)
            else:
                row.append('N/A')
        
        # è®¡ç®—è¯¥æ•°æ®é›†çš„å¹³å‡å€¼
        if dataset_accs:
            dataset_avg = sum(dataset_accs) / len(dataset_accs)
            row.append(f'{dataset_avg:.4f}')
        else:
            row.append('N/A')
        
        table.add_row(row)
    
    # æ·»åŠ å¹³å‡å€¼è¡Œ
    avg_row = ['Average']
    for noise in noise_list:
        noise_accs = []
        for dataset in datasets:
            if dataset in results and results[dataset].get(noise) is not None:
                noise_accs.append(results[dataset][noise])
        
        if noise_accs:
            avg_row.append(f'{sum(noise_accs) / len(noise_accs):.4f}')
        else:
            avg_row.append('N/A')
    
    # æœ€åä¸€è¡Œçš„å¹³å‡å€¼åˆ—ï¼ˆä¸è®¡ç®—æ€»å¹³å‡å€¼ï¼‰
    avg_row.append('N/A')
    
    table.add_row(avg_row)
    
    # è¾“å‡ºè¡¨æ ¼
    print("\n" + "=" * 80)
    print(f"ğŸ“Š å®éªŒ3 (MIAæ”»å‡») ç»“æœè¡¨æ ¼ - {exp_name}")
    print("=" * 80)
    print(table)
    print("=" * 80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
    has_results = any(
        results.get(dataset, {}).get(noise) is not None
        for dataset in datasets
        for noise in noise_list
    )
    if not has_results:
        print("\nâš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
    
    # ä¿å­˜è¡¨æ ¼åˆ°æ–‡ä»¶
    output_file = exp_dir / 'mia_results_table.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(f"å®éªŒ3 (MIAæ”»å‡») ç»“æœè¡¨æ ¼ - {exp_name}\n")
        f.write("=" * 80 + "\n")
        f.write(str(table))
        f.write("\n" + "=" * 80 + "\n")
    
    print(f"\nğŸ’¾ è¡¨æ ¼å·²ä¿å­˜åˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="SepFPL å®éªŒç»“æœç”Ÿæˆå·¥å…·", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    exp_group = parser.add_argument_group("å®éªŒé…ç½®")
    for arg_name, (_, desc) in EXP_ARG_MAP.items():
        exp_group.add_argument(f"--{arg_name.replace('_', '-')}", action="store_true", help=desc)

    parser.add_argument("--output-dir", type=Path, default=DEFAULT_OUTPUT_DIR, help="æ•°æ®ç›®å½•")
    parser.add_argument("--tail-epochs", type=int, default=DEFAULT_TAIL_EPOCHS, help="ç»Ÿè®¡è½®æ¬¡")
    parser.add_argument("--output-file", type=Path, default=None, 
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆåŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œåªè¾“å‡ºåˆ°ç»ˆç«¯")
    parser.add_argument("--no-postprocess", action="store_true", 
                       help="ç¦ç”¨åå¤„ç†ï¼Œè¾“å‡ºåŸå§‹æ•°æ®è¡¨æ ¼ï¼ˆé»˜è®¤å¯ç”¨åå¤„ç†ï¼‰")
    parser.add_argument("--mia-only", action="store_true",
                       help="ä»…ç”Ÿæˆå®éªŒ3 (MIA) çš„ç»“æœè¡¨æ ¼")
    parser.add_argument("--mia-exp-name", type=str, default='exp3-mia',
                       help="MIAå®éªŒç»„åï¼ˆé»˜è®¤: exp3-miaï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    output_file = None
    if args.output_file:
        output_file = args.output_file
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ‰©å±•åï¼Œé»˜è®¤ä½¿ç”¨ .txt
        if not output_file.suffix:
            output_file = output_file.with_suffix('.txt')
        print(f"ğŸ“ è¾“å‡ºå°†åŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")
    
    # ä½¿ç”¨ TeeOutput åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶
    with TeeOutput(output_file) as tee:
        if output_file:
            sys.stdout = tee
        
        # å¦‚æœæŒ‡å®šäº† --mia-onlyï¼Œåªç”ŸæˆMIAè¡¨æ ¼
        if args.mia_only:
            generate_mia_table(
                exp_name=args.mia_exp_name,
                output_dir=args.output_dir,
                datasets=None,  # è‡ªåŠ¨æ‰«æ
                noise_list=None  # è‡ªåŠ¨æ‰«æ
            )
        else:
            # åŸæœ‰çš„å®éªŒ1å’Œå®éªŒ2è¡¨æ ¼ç”Ÿæˆé€»è¾‘
            configs_to_run = []
            any_flag = False
            for arg_attr in EXP_ARG_MAP.keys():
                if getattr(args, arg_attr, False):
                    any_flag = True
                    for key in EXP_ARG_MAP[arg_attr][0]:
                        if key not in configs_to_run: configs_to_run.append(key)
            
            if not any_flag:
                configs_to_run = list(EXPERIMENT_CONFIGS.keys())

            enable_postprocess = not args.no_postprocess  # é»˜è®¤å¯ç”¨åå¤„ç†
            
            for key in configs_to_run:
                if key in EXPERIMENT_CONFIGS:
                    generate_tables(key, EXPERIMENT_CONFIGS[key], args.output_dir, args.tail_epochs, enable_postprocess)
        
        if output_file:
            sys.stdout = tee.console
            print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")

if __name__ == "__main__":
    main()