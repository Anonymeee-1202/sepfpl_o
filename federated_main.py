"""
è”é‚¦å­¦ä¹ ä¸»ç¨‹åºï¼šæ”¯æŒå¤šç§çŸ©é˜µåˆ†è§£ç®—æ³•å’Œå·®åˆ†éšç§æœºåˆ¶

ä¸»è¦åŠŸèƒ½ï¼š
- è”é‚¦æç¤ºå­¦ä¹ ï¼ˆFederated Prompt Learningï¼‰
- æ”¯æŒå¤šç§ç®—æ³•å˜ä½“ï¼šsepfpl, dpfpl, fedotp, fedpgp, dplora ç­‰
- æ”¯æŒ HCSEï¼ˆåˆ†å±‚èšç±»ç»“æ„ç†µï¼‰å’Œæ—¶é—´è‡ªé€‚åº”éšç§åˆ†é…
- é›†æˆ wandb å®éªŒè·Ÿè¸ª
"""

import argparse
import torch
from Dassl.dassl.utils import set_random_seed
from Dassl.dassl.config import get_cfg_default
from Dassl.dassl.engine import build_trainer

import os
import math
import copy
import pickle
import numpy as np
import time
from tqdm import tqdm
from prettytable import PrettyTable
from utils.logger import init_logger_from_args
from utils.config_utils import setup_cfg
from utils.checkpoint_utils import (
    build_filename_suffix,
    get_output_dir,
    save_checkpoint,
    load_checkpoint,
)

try:
    import wandb  # type: ignore
    _WANDB_AVAILABLE = True
except ImportError:
    wandb = None
    _WANDB_AVAILABLE = False


def _should_enable_wandb(args):
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥å¯ç”¨ wandbï¼š
    - è‹¥ç¯å¢ƒå˜é‡ WANDB_DISABLED è®¾ç½®ä¸º 1/true/yesï¼Œåˆ™å¼ºåˆ¶ç¦ç”¨ã€‚
    """
    env_disabled = os.environ.get('WANDB_DISABLED', '').lower() in ['1', 'true', 'yes']
    if env_disabled:
        return False
    return True


def _default_wandb_run_name(args):
    """
    ä¸ºå½“å‰å®éªŒç”Ÿæˆä¸€ä¸ªé»˜è®¤çš„ wandb è¿è¡Œåç§°ï¼Œä¾¿äºåŒºåˆ†ã€‚
    ç¤ºä¾‹ï¼šcifar100-sepfpl-rank8-noise0.1-seed1-users10-[1/100]
    """
    dataset = os.path.splitext(os.path.basename(args.dataset_config_file))[0]
    parts = [
        dataset,
        args.factorization,
        f"rank{args.rank}",
        f"noise{args.noise}",
        f"seed{args.seed}",
        f"users{args.num_users}",
    ]
    if args.task_id:
        parts.append(args.task_id.strip('[]'))
    return "-".join(parts)


def _prepare_wandb_tags(args):
    """
    ç”Ÿæˆ wandb ç”¨çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆtagsï¼‰ï¼Œç”¨äºè¿‡æ»¤ä¸å¯¹æ¯”å®éªŒã€‚
    """
    dataset = os.path.splitext(os.path.basename(args.dataset_config_file))[0]
    tags = {
        f"dataset:{dataset}",
        f"factorization:{args.factorization}",
        f"rank:{args.rank}",
        f"noise:{args.noise}",
        f"users:{args.num_users}",
    }
    if args.task_id:
        tags.add(f"task:{args.task_id}")
    return sorted(tags)


def _use_hcse(args) -> bool:
    """
    æ ¹æ® factorization åç§°åˆ¤æ–­å½“å‰æ˜¯å¦å¯ç”¨ HCSEï¼ˆHierarchical Clustering based Structural Entropyï¼‰ã€‚

    - sepfpl / sepfpl_hcseï¼šå¯ç”¨ HCSEï¼›
    - å…¶ä»–ç®—æ³•ï¼šé»˜è®¤å…³é—­ã€‚
    """
    return args.factorization in ('sepfpl', 'sepfpl_hcse')


def _use_time_adaptive(args) -> bool:
    """
    æ ¹æ® factorization åç§°åˆ¤æ–­æ˜¯å¦å¯ç”¨æ—¶é—´è‡ªé€‚åº”éšç§åˆ†é…ã€‚

    - sepfpl / sepfpl_time_adaptiveï¼šå¯ç”¨æ—¶é—´è‡ªé€‚åº”ï¼›
    - å…¶ä»–ç®—æ³•ï¼šé»˜è®¤å…³é—­ã€‚
    """
    return args.factorization in ('sepfpl', 'sepfpl_time_adaptive')


def init_wandb_run(args, cfg, logger):
    """
    åˆå§‹åŒ– wandb è¿è¡Œï¼ˆè‹¥å¯ç”¨ä¸”æœªè¢«ç¦ç”¨ï¼‰ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        cfg: é…ç½®å¯¹è±¡
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        wandb.Run å¯¹è±¡ï¼Œè‹¥ä¸å¯ç”¨æˆ–å·²ç¦ç”¨åˆ™è¿”å› None
    """
    if not _WANDB_AVAILABLE:
        if _should_enable_wandb(args):
            logger.warning("å·²è¯·æ±‚ä½¿ç”¨ Weights & Biasesï¼Œä½†æœªå®‰è£… wandb åŒ…ï¼Œå·²è‡ªåŠ¨ç¦ç”¨ã€‚")
        return None
    if not _should_enable_wandb(args):
        return None

    project = 'SepFPL'
    group = args.wandb_group
    mode = 'offline'
    run_name = _default_wandb_run_name(args)
    tags = _prepare_wandb_tags(args)

    # è®°å½•å…³é”®è¶…å‚æ•°ï¼Œæ–¹ä¾¿åœ¨ wandb ç•Œé¢ä¸­æŸ¥çœ‹ä¸å¯¹æ¯”
    config_payload = {
        "dataset_config": args.dataset_config_file,
        "factorization": args.factorization,
        "rank": args.rank,
        "noise": args.noise,
        "rdp_alpha": args.rdp_alpha,
        "rdp_p": args.rdp_p,
        "num_users": args.num_users,
        "round": args.round,
        "seed": args.seed,
        "task_id": args.task_id,
        "norm_thresh": args.norm_thresh,
        "lr": args.lr,
        "train_batch_size": args.train_batch_size,
        "test_batch_size": args.test_batch_size,
        "partition": args.partition,
        "beta": args.beta,
        "n_ctx": args.n_ctx,
    }

    init_kwargs = {
        "project": project,
        "group": group,
        "mode": mode,
        "name": run_name,
        "tags": tags,
        "config": config_payload,
        "dir": os.path.expanduser('~/code/sepfpl/'),
        "settings": wandb.Settings(start_method="thread", _disable_stats=True),
    }
    # å»é™¤å€¼ä¸º None çš„é”®ï¼Œä»¥é¿å… wandb æŠ¥é”™
    init_kwargs = {k: v for k, v in init_kwargs.items() if v is not None}

    logger.info(f"[wandb] æ­£åœ¨åˆå§‹åŒ–å®éªŒï¼šproject={project}, name={run_name}, mode={mode}")
    return wandb.init(**init_kwargs)


def initialize_prompt_buffers(args, local_weights, use_hcse_flag):
    """
    åˆå§‹åŒ–æç¤ºå‚æ•°ç¼“å†²åŒºæ˜ å°„ã€‚
    
    æ ¹æ®ä¸åŒçš„ç®—æ³•å˜ä½“ï¼Œå†³å®šéœ€è¦ç»´æŠ¤å“ªäº›æç¤ºå‚æ•°ç¼“å†²åŒºï¼š
    - global_ctx: æ‰€æœ‰ç®—æ³•éƒ½éœ€è¦
    - local_ctx: fedotp, dplora, dpfpl, sepfpl ç³»åˆ—
    - local_u_ctx / local_v_ctx: fedpgp, dplora, dpfpl, sepfpl ç³»åˆ—
    - cluster_ctx: ä»… HCSE æ¨¡å¼éœ€è¦
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        local_weights: å®¢æˆ·ç«¯æƒé‡åˆ—è¡¨
        use_hcse_flag: æ˜¯å¦å¯ç”¨ HCSE
        
    Returns:
        dict: æç¤ºå‚æ•°é”®åˆ°ç¼“å†²åŒºçš„æ˜ å°„
    """
    local_weights_g = [None for _ in range(args.num_users)]
    local_weights_l = [None for _ in range(args.num_users)]
    local_weights_u = [None for _ in range(args.num_users)]
    local_weights_v = [None for _ in range(args.num_users)]
    
    key_to_buffer = {'prompt_learner.global_ctx': local_weights_g}
    
    # æ ¹æ®ç®—æ³•å˜ä½“å†³å®šç»´æŠ¤å“ªäº›æç¤ºå‚æ•°
    if args.factorization in ['fedotp', 'dplora', 'dpfpl', 'sepfpl', 'sepfpl_time_adaptive', 'sepfpl_hcse']:
        key_to_buffer['prompt_learner.local_ctx'] = local_weights_l
    if args.factorization in ['fedpgp', 'dplora', 'dpfpl', 'sepfpl', 'sepfpl_time_adaptive', 'sepfpl_hcse']:
        key_to_buffer['prompt_learner.local_u_ctx'] = local_weights_u
        key_to_buffer['prompt_learner.local_v_ctx'] = local_weights_v
    if use_hcse_flag:
        key_to_buffer['prompt_learner.cluster_ctx'] = [
            copy.deepcopy(local_weights[i]['prompt_learner.cluster_ctx'])
            if 'prompt_learner.cluster_ctx' in local_weights[i] else None
            for i in range(args.num_users)
        ]
    
    return key_to_buffer


def collect_client_gradients(local_trainer, idxs_users, data_iters, batch, max_batch,
                             initial_weights, local_weights, global_gradients, 
                             key_to_buffer, use_hcse, epoch):
    """
    æ”¶é›†æ‰€æœ‰å®¢æˆ·ç«¯çš„æ¢¯åº¦å¹¶æ›´æ–°ç¼“å†²åŒºã€‚
    
    Args:
        local_trainer: æœ¬åœ°è®­ç»ƒå™¨
        idxs_users: å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
        data_iters: æ•°æ®è¿­ä»£å™¨åˆ—è¡¨
        batch: å½“å‰æ‰¹æ¬¡ç´¢å¼•
        max_batch: æ¯è½®æœ€å¤§æ‰¹æ¬¡æ•°é‡
        initial_weights: åˆå§‹æ¨¡å‹æƒé‡
        local_weights: å®¢æˆ·ç«¯æƒé‡åˆ—è¡¨
        global_gradients: å…¨å±€æ¢¯åº¦åˆ—è¡¨ï¼ˆè¾“å‡ºï¼‰
        key_to_buffer: æç¤ºå‚æ•°åˆ°ç¼“å†²åŒºçš„æ˜ å°„
        use_hcse: æ˜¯å¦ä½¿ç”¨ HCSE
        epoch: å½“å‰è½®æ¬¡
        
    Returns:
        tuple: (cluster_grads, train_acc_sum, train_acc_count)
            - cluster_grads: èšç±»æ¢¯åº¦åˆ—è¡¨ï¼ˆHCSE æ¨¡å¼ï¼‰æˆ– None
            - train_acc_sum: è®­ç»ƒç²¾åº¦ç´¯è®¡å’Œ
            - train_acc_count: è®­ç»ƒç²¾åº¦è®¡æ•°
    """
    cluster_grads = [None for _ in idxs_users] if use_hcse else None
    train_acc_sum = 0.0
    train_acc_count = 0
    
    for idx in idxs_users:
        # åŠ è½½å®¢æˆ·ç«¯æƒé‡
        if epoch == 0:
            local_trainer.model.load_state_dict(initial_weights, strict=False)
        else:
            local_trainer.model.load_state_dict(local_weights[idx], strict=False)
        
        # å‰å‘ä¼ æ’­
        loss_summary = local_trainer.train_forward(
            idx=idx,
            train_iter=data_iters[idx],
            current_batch=batch,
            total_batches=max_batch
        )
        
        # ç´¯ç§¯è®­ç»ƒç²¾åº¦ç»Ÿè®¡
        if loss_summary is not None and 'acc' in loss_summary:
            train_acc_sum += loss_summary['acc']
            train_acc_count += 1
        
        # æå–å…¨å±€ prompt æ¢¯åº¦
        local_weight = local_trainer.model.state_dict()
        grad_global = local_trainer.model.prompt_learner.global_ctx.grad
        if grad_global is not None:
            global_gradients[idx] = grad_global.data.clone()
        else:
            global_gradients[idx] = torch.zeros_like(
                local_trainer.model.prompt_learner.global_ctx.data
            )
        
        # å°†æç¤ºå‚æ•°å†™å…¥ç¼“å†²åŒº
        for key, buffer in key_to_buffer.items():
            if buffer is None or key not in local_weight:
                continue
            buffer[idx] = copy.deepcopy(local_weight[key])
        
        # HCSEï¼šè®°å½• cluster_ctx æ¢¯åº¦
        if use_hcse and 'prompt_learner.cluster_ctx' in local_weight:
            if local_trainer.model.prompt_learner.cluster_ctx.grad is not None:
                cluster_grads[idx] = local_trainer.model.prompt_learner.cluster_ctx.grad.data.clone()
            else:
                cluster_grads[idx] = torch.zeros_like(
                    local_trainer.model.prompt_learner.cluster_ctx.data
                )
    
    return cluster_grads, train_acc_sum, train_acc_count


def aggregate_gradients_with_hcse(cluster_grads, global_gradients, idxs_users, 
                                   local_trainer, args, logger):
    """
    ä½¿ç”¨ HCSE æ–¹æ³•èšåˆæ¢¯åº¦ã€‚
    
    åŒ…æ‹¬ï¼š
    1. æŒ‰æ•°æ®é‡åŠ æƒèšåˆå…¨å±€æ¢¯åº¦
    2. è®¡ç®—æ¢¯åº¦ç›¸ä¼¼åº¦çŸ©é˜µ
    3. Top-k ç¨€ç–åŒ–ã€å¯¹ç§°åŒ–ã€æŒ‡æ•°æ˜ å°„
    4. æ„å»ºç¼–ç æ ‘å¹¶èšåˆèšç±»æ¢¯åº¦
    
    Args:
        cluster_grads: èšç±»æ¢¯åº¦åˆ—è¡¨
        global_gradients: å…¨å±€æ¢¯åº¦åˆ—è¡¨
        idxs_users: å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
        local_trainer: æœ¬åœ°è®­ç»ƒå™¨
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        tuple: (avg_global_gradient, cluster_gradients_to_apply)
            - avg_global_gradient: èšåˆåçš„å…¨å±€æ¢¯åº¦
            - cluster_gradients_to_apply: åˆ†é…ç»™å„å®¢æˆ·ç«¯çš„èšç±»æ¢¯åº¦å­—å…¸
    """
    # æ„å»ºåŠ æƒå…¨å±€æ¢¯åº¦ï¼ˆæŒ‰å„å®¢æˆ·ç«¯æ•°æ®é‡åŠ æƒï¼‰
    per_user_global = []
    data_sizes = []
    for i in idxs_users:
        try:
            ds_len = len(local_trainer.fed_train_loader_x_dict[i].dataset)
        except Exception:
            ds_len = 1
        data_sizes.append(max(1, ds_len))
        per_user_global.append(global_gradients[i])
    total_size = float(sum(data_sizes))
    weights = [s / total_size for s in data_sizes]
    avg_global_gradient = sum(w * g for w, g in zip(weights, per_user_global))
    
    # åŸºäº HCSE çš„æ¢¯åº¦èšç±»ä¸ç¼–ç æ ‘èšåˆ
    cluster_gradients_to_apply = None
    try:
        from hcse.encoding_tree import (
            PartitionTree,
            compute_gradient_similarity_matrix_torch,
            aggregate_gradients_by_encoding_tree,
        )
        
        # å¡«å……ç¼ºå¤±çš„ cluster_grad
        for i in range(len(cluster_grads)):
            if cluster_grads[i] is None:
                if 'prompt_learner.cluster_ctx' in local_trainer.model.state_dict():
                    zshape = local_trainer.model.prompt_learner.cluster_ctx.data.shape
                    device = local_trainer.model.prompt_learner.cluster_ctx.data.device
                    cluster_grads[i] = torch.zeros(zshape, device=device)
                else:
                    cluster_grads[i] = torch.zeros_like(
                        local_trainer.model.prompt_learner.global_ctx.data
                    )
        
        # è®¡ç®—æ¢¯åº¦ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_mat = compute_gradient_similarity_matrix_torch(cluster_grads, normalize=True)
        
        # Top-k ç¨€ç–åŒ– + å¯¹ç§°åŒ– + æŒ‡æ•°æ˜ å°„
        k = args.sepfpl_topk if hasattr(args, 'sepfpl_topk') and args.sepfpl_topk is not None else 5
        with torch.no_grad():
            sim_proc = sim_mat.clone()
            n = sim_proc.shape[0]
            for r in range(n):
                vals, idxs = torch.topk(sim_proc[r], k=k, largest=True)
                mask = torch.zeros_like(sim_proc[r], dtype=torch.bool)
                mask[idxs] = True
                mask[r] = True
                sim_proc[r][~mask] = 0.0
            # å¯¹ç§°åŒ–
            sim_proc = torch.maximum(sim_proc, sim_proc.t())
            # æŒ‡æ•°æ˜ å°„å¢å¼ºå¯¹é«˜ç›¸ä¼¼åº¦è¾¹çš„åŒºåˆ†
            sim_proc = torch.exp(sim_proc)
        
        adj_matrix = sim_proc.detach().cpu().numpy()
        # åŸºäºç›¸ä¼¼åº¦å›¾æ„å»ºç¼–ç æ ‘
        tree = PartitionTree(adj_matrix)
        tree.build_encoding_tree(k=4, mode='v2')
        
        # æ²¿ç¼–ç æ ‘èšåˆ cluster_ctx æ¢¯åº¦
        aggregated_cluster_grads = aggregate_gradients_by_encoding_tree(
            tree, cluster_grads, adj_matrix
        )
        
        # å°†èšåˆåçš„æ¢¯åº¦åˆ†é…ç»™å„å®¢æˆ·ç«¯
        cluster_gradients_to_apply = {
            i: aggregated_cluster_grads[i]
            for i in idxs_users
            if aggregated_cluster_grads[i] is not None
        }
        
        # è¿”å›ç¼–ç æ ‘å’Œé‚»æ¥çŸ©é˜µç”¨äºæ¢¯åº¦èšç±»å¯è§†åŒ–
        return avg_global_gradient, cluster_gradients_to_apply, tree, adj_matrix
    except Exception as e:
        logger.warning(f"[HCSE] èšç±»ä¸èšåˆå‡ºç°å¼‚å¸¸ï¼Œè·³è¿‡æœ¬æ­¥: {e}")
        tree = None
        adj_matrix = None
    
    return avg_global_gradient, cluster_gradients_to_apply, tree, adj_matrix


def visualize_encoding_tree(tree, adj_matrix, output_dir, epoch, args, logger):
    """
    Visualize encoding tree structure and save as PDF.
    
    Args:
        tree: PartitionTree object
        adj_matrix: Adjacency matrix used to build the tree
        output_dir: Output directory path
        epoch: Current epoch number
        args: Command line arguments
        logger: Logger object
        
    Returns:
        bool: True if visualization succeeded, False otherwise
    """
    if tree is None or adj_matrix is None:
        return False
    
    try:
        from hcse.visualization import EncodingTreeVisualizer
        import matplotlib
        matplotlib.use('Agg')  # Use non-interactive backend to avoid plt.show() blocking
        import matplotlib.pyplot as plt
        
        # Create visualizer
        visualizer = EncodingTreeVisualizer(tree, adj_matrix)
        filename_suffix = build_filename_suffix(args, prefix='')
        tree_save_path = output_dir / f'encoding_tree_e{epoch+1}_{filename_suffix}.pdf'
        
        # Create visualization figure and save as PDF
        # Note: visualize_tree_structure will call plt.show(), but it won't display with Agg backend
        fig = visualizer.visualize_tree_structure(
            figsize=(100, 150),
            save_path=str(tree_save_path)  # Save as PDF (format auto-detected by extension)
        )
        if fig is not None:
            # Ensure file is saved (visualize_tree_structure saves internally, but save again for safety)
            if not tree_save_path.exists() or tree_save_path.stat().st_size == 0:
                fig.savefig(tree_save_path, format='pdf', dpi=300, bbox_inches='tight')
            plt.close(fig)  # Close figure to release memory
            logger.info(f"âœ… Encoding tree visualization saved: {tree_save_path}")
            return True
        return False
    except Exception as e:
        logger.warning(f"âš ï¸ Encoding tree visualization failed: {e}")
        return False


def collect_gradient_clustering_data(cluster_grads, idxs_users, local_trainer, tree, cfg, epoch, logger=None):
    """
    æ”¶é›†æ¢¯åº¦èšç±»å¯è§†åŒ–æ‰€éœ€çš„æ•°æ®
    
    Args:
        cluster_grads: ç°‡çº§æ¢¯åº¦åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ torch.Tensorï¼ˆcluster_ctx çš„æ¢¯åº¦ï¼‰
        idxs_users: å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
        local_trainer: æœ¬åœ°è®­ç»ƒå™¨
        tree: ç¼–ç æ ‘å¯¹è±¡ï¼ˆå¯èƒ½ä¸º Noneï¼‰ï¼Œç”¨äºè·å–ç¤¾åŒºåˆ’åˆ†
        cfg: é…ç½®å¯¹è±¡
        epoch: å½“å‰è½®æ¬¡ï¼ˆ0-indexedï¼‰
        logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤çš„ logger
        
    Returns:
        dict: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
            - 'gradient_vectors': list[numpy.ndarray] - æ¯ä¸ªå®¢æˆ·ç«¯çš„æ¢¯åº¦å‘é‡ï¼ˆflattenåï¼‰
            - 'client_labels': list[int] - æ¯ä¸ªå®¢æˆ·ç«¯æ•°æ®ä¸­å ä¸»å¯¼åœ°ä½çš„ç±»åˆ«ID
            - 'client_classnames': list[str] - æ¯ä¸ªå®¢æˆ·ç«¯æ•°æ®ä¸­å ä¸»å¯¼åœ°ä½çš„ç±»åˆ«åç§°
            - 'community_ids': list[int] - æ¯ä¸ªå®¢æˆ·ç«¯æ‰€å±çš„ç¤¾åŒºIDï¼ˆ-1è¡¨ç¤ºæœªåˆ†é…ï¼‰
            - 'client_ids': list[int] - å®¢æˆ·ç«¯IDåˆ—è¡¨
            
        æ³¨æ„: gradient_vectors å’Œ client_ids åªåŒ…å« cluster_grads[idx] ä¸ä¸º None çš„å®¢æˆ·ç«¯
    """
    if logger is None:
        import logging
        logger = logging.getLogger(__name__)
    
    if not cfg.GRADIENT_CLUSTERING:
        logger.debug("âŒ æ¢¯åº¦èšç±»æœªå¯ç”¨: cfg.GRADIENT_CLUSTERING=False")
        return None
    
    # ä¿®æ”¹ï¼šæ¯ä¸ª epoch éƒ½æ”¶é›†æ•°æ®ï¼Œè€Œä¸æ˜¯åªåœ¨æœ€åä¸€è½®
    # å¦‚æœéœ€è¦åªåœ¨ç‰¹å®šè½®æ¬¡æ”¶é›†ï¼Œå¯ä»¥é€šè¿‡é…ç½®æ§åˆ¶
    # target_epoch = cfg.OPTIM.ROUND - 1  # æœ€åä¸€è½®ï¼ˆ0-indexedï¼‰
    # if epoch != target_epoch:
    #     return None
    
    logger.debug(f"ğŸ” æ”¶é›†æ¢¯åº¦èšç±»æ•°æ®: epoch={epoch}, cfg.OPTIM.ROUND={cfg.OPTIM.ROUND}")
    
    data = {
        'gradient_vectors': [],
        'client_labels': [],
        'client_classnames': [],
        'community_ids': [],
        'client_ids': []
    }
    
    # 1. æ”¶é›†æ¢¯åº¦å‘é‡ï¼ˆflattenï¼‰
    for idx in idxs_users:
        if cluster_grads[idx] is not None:
            grad_vec = cluster_grads[idx].detach().cpu().flatten().numpy()
            data['gradient_vectors'].append(grad_vec)
            data['client_ids'].append(idx)
        else:
            continue
    
    # 2. æ”¶é›†å®¢æˆ·ç«¯æ•°æ®ç±»åˆ«æ ‡ç­¾ï¼ˆå ä¸»å¯¼åœ°ä½çš„ç±»åˆ«ï¼‰
    for idx in idxs_users:
        try:
            # è·å–å®¢æˆ·ç«¯çš„æ•°æ®é›†
            dataset_wrapper = local_trainer.fed_train_loader_x_dict[idx].dataset
            # DatasetWrapper æœ‰ä¸€ä¸ª data_source å±æ€§ï¼ŒåŒ…å«åŸå§‹çš„ Datum å¯¹è±¡åˆ—è¡¨
            if hasattr(dataset_wrapper, 'data_source'):
                data_source = dataset_wrapper.data_source
            elif hasattr(dataset_wrapper, 'dataset') and hasattr(dataset_wrapper.dataset, 'data_source'):
                data_source = dataset_wrapper.dataset.data_source
            else:
                # å°è¯•ç›´æ¥è®¿é—®
                data_source = dataset_wrapper
            
            # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
            from collections import Counter
            labels = [item.label for item in data_source]
            if len(labels) == 0:
                raise ValueError(f"å®¢æˆ·ç«¯ {idx} çš„æ•°æ®æºä¸ºç©º")
            label_counter = Counter(labels)
            # è·å–å ä¸»å¯¼åœ°ä½çš„ç±»åˆ«
            dominant_label = label_counter.most_common(1)[0][0]
            data['client_labels'].append(dominant_label)
            
            # è·å–å ä¸»å¯¼åœ°ä½çš„ç±»åˆ«å¯¹åº”çš„ classname
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå…·æœ‰ dominant_label çš„ item
            dominant_classname = None
            for item in data_source:
                if item.label == dominant_label:
                    dominant_classname = item.classname
                    break
            data['client_classnames'].append(dominant_classname if dominant_classname else "")
        except Exception as e:
            # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨ -1 ä½œä¸ºé»˜è®¤å€¼
            logger.debug(f"âš ï¸ å®¢æˆ·ç«¯ {idx} æ— æ³•è·å–æ ‡ç­¾: {e}")
            data['client_labels'].append(-1)
            data['client_classnames'].append("")
    
    # 3. æ”¶é›†ç¤¾åŒºIDï¼ˆä»ç¼–ç æ ‘ä¸­è·å–ï¼‰
    if tree is not None:
        try:
            # è·å–ç¼–ç æ ‘çš„ç°‡åˆ’åˆ†
            # ä¸‹æ½œåˆ°å¶èŠ‚ç‚¹ï¼šæ¯ä¸ªå¶èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹æ‰€åŒ…å«çš„æ‰€æœ‰å¶èŠ‚ç‚¹ï¼Œç»„æˆä¸€ä¸ªcluster
            clusters = []
            if tree.root_id is not None:
                # æ‰¾åˆ°æ‰€æœ‰å¶èŠ‚ç‚¹ï¼ˆæ²¡æœ‰å­èŠ‚ç‚¹æˆ–å­èŠ‚ç‚¹ä¸ºç©ºçš„èŠ‚ç‚¹ï¼‰
                leaf_nodes = []
                for node_id, node in tree.tree_node.items():
                    if not node.children or len(node.children) == 0:
                        leaf_nodes.append(node_id)
                
                # æ ¹æ®å¶èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹è¿›è¡Œåˆ†ç»„
                parent_to_leaves = {}
                for leaf_id in leaf_nodes:
                    leaf_node = tree.tree_node[leaf_id]
                    parent_id = leaf_node.parent
                    if parent_id is not None:
                        if parent_id not in parent_to_leaves:
                            parent_to_leaves[parent_id] = []
                        # å¶èŠ‚ç‚¹çš„ partition åªåŒ…å«ä¸€ä¸ªåŸå§‹å›¾èŠ‚ç‚¹
                        parent_to_leaves[parent_id].extend(leaf_node.partition)
                
                # æ¯ä¸ªçˆ¶èŠ‚ç‚¹ä¸‹çš„æ‰€æœ‰å¶èŠ‚ç‚¹ç»„æˆä¸€ä¸ªcluster
                clusters = [list(leaves) for leaves in parent_to_leaves.values() if leaves]
            
            # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…ç¤¾åŒºID
            client_to_community = {}
            for comm_id, cluster in enumerate(clusters):
                for client_idx in cluster:
                    client_to_community[client_idx] = comm_id
            
            # æŒ‰ç…§ idxs_users çš„é¡ºåºè·å–ç¤¾åŒºID
            for idx in idxs_users:
                comm_id = client_to_community.get(idx, -1)
                data['community_ids'].append(comm_id)
        except Exception as e:
            # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨ -1 ä½œä¸ºé»˜è®¤å€¼
            data['community_ids'] = [-1] * len(idxs_users)
    else:
        data['community_ids'] = [-1] * len(idxs_users)
    
    # ä½¿ç”¨ PrettyTable è¾“å‡ºæ•°æ®
    # è¡¨æ ¼ç»“æ„ï¼šç¬¬1åˆ—æ˜¯community_idsï¼ˆé€’å¢ï¼‰ï¼Œç¬¬2åˆ—æ˜¯client_idsï¼Œç¬¬3åˆ—æ˜¯client_labelsï¼Œç¬¬4åˆ—æ˜¯client_classnames
    logger.debug(f"æ•°æ®æ”¶é›†å®Œæˆ: community_idsæ•°é‡={len(data['community_ids'])}, client_idsæ•°é‡={len(data['client_ids'])}")
    if len(data['client_ids']) > 0:
        # åˆ›å»ºå®¢æˆ·ç«¯æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« (community_id, client_id, client_label, client_classname)
        client_data_list = []
        
        # è·å– client_ids å¯¹åº”çš„ç´¢å¼•åœ¨åŸå§‹ idxs_users ä¸­çš„ä½ç½®
        for i, client_id in enumerate(data['client_ids']):
            # æ‰¾åˆ°è¿™ä¸ª client_id åœ¨åŸå§‹ idxs_users ä¸­çš„ä½ç½®
            if client_id in idxs_users:
                idx_pos = idxs_users.index(client_id)
                # è·å–å¯¹åº”çš„ community_id
                if idx_pos < len(data['community_ids']):
                    comm_id = data['community_ids'][idx_pos]
                else:
                    comm_id = -1
                
                # è·å–å¯¹åº”çš„ client_label å’Œ client_classname
                if idx_pos < len(data['client_labels']):
                    client_label = data['client_labels'][idx_pos]
                else:
                    client_label = -1
                
                if idx_pos < len(data['client_classnames']):
                    client_classname = data['client_classnames'][idx_pos]
                else:
                    client_classname = ""
                
                client_data_list.append((comm_id, client_id, client_label, client_classname))
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                client_data_list.append((-1, client_id, -1, ""))
        
        # æŒ‰ community_id æ’åºï¼ˆç¬¬ä¸€åˆ—ï¼Œé€’å¢ï¼‰
        client_data_list.sort(key=lambda x: x[0])
        
        # åˆ›å»ºè¡¨æ ¼
        table = PrettyTable()
        table.field_names = ['community_id', 'client_id', 'client_label', 'client_classname']
        table.align = "l"
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯æ·»åŠ ä¸€è¡Œ
        for comm_id, client_id, client_label, client_classname in client_data_list:
            table.add_row([comm_id, client_id, client_label, client_classname])
        
        # ä½¿ç”¨ print ç¡®ä¿è¡¨æ ¼èƒ½ç›´æ¥è¾“å‡ºï¼ŒåŒæ—¶ä¹Ÿä½¿ç”¨ logger
        print(f"\næ¢¯åº¦èšç±»æ•°æ®æ±‡æ€» (epoch={epoch}):\n{table}")
        logger.info(f"\næ¢¯åº¦èšç±»æ•°æ®æ±‡æ€» (epoch={epoch}):\n{table}")
    else:
        warning_msg = "âš ï¸ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®"
        print(warning_msg)
        logger.warning(warning_msg)
    
    return data


def apply_differential_privacy_noise(avg_global_gradient, cluster_gradients_to_apply, std):
    """
    ä¸ºèšåˆåçš„æ¢¯åº¦æ·»åŠ å·®åˆ†éšç§é«˜æ–¯å™ªå£°ã€‚
    
    Args:
        avg_global_gradient: èšåˆåçš„å…¨å±€æ¢¯åº¦
        cluster_gradients_to_apply: èšç±»æ¢¯åº¦å­—å…¸ï¼ˆå¯èƒ½ä¸º Noneï¼‰
        std: å™ªå£°æ ‡å‡†å·®
        
    Returns:
        tuple: (noisy_avg_global_gradient, noisy_cluster_gradients_to_apply)
    """
    # ä¸ºå…¨å±€æ¢¯åº¦æ·»åŠ å™ªå£°
    noise = torch.normal(
        0, std,
        size=avg_global_gradient.shape,
        device=avg_global_gradient.device
    )
    avg_global_gradient += noise
    
    # ä¸ºèšç±»æ¢¯åº¦æ·»åŠ å™ªå£°
    if cluster_gradients_to_apply is not None:
        for idx in cluster_gradients_to_apply:
            cluster_grad = cluster_gradients_to_apply[idx]
            if cluster_grad is not None:
                cluster_noise = torch.normal(
                    0, std,
                    size=cluster_grad.shape,
                    device=cluster_grad.device
                )
                cluster_gradients_to_apply[idx] = cluster_grad + cluster_noise
    
    return avg_global_gradient, cluster_gradients_to_apply


def update_client_weights(local_trainer, idxs_users, local_weights, initial_weights,
                          key_to_buffer, avg_global_gradient, cluster_gradients_to_apply,
                          get_prompt_default):
    """
    æ›´æ–°æ‰€æœ‰å®¢æˆ·ç«¯çš„æƒé‡ã€‚
    
    å¯¹æ¯ä¸ªå®¢æˆ·ç«¯ï¼š
    1. ä»ç¼“å†²åŒºåŒæ­¥æç¤ºå‚æ•°åˆ° local_weights
    2. åŠ è½½æƒé‡åˆ°æ¨¡å‹
    3. æ‰§è¡Œåå‘ä¼ æ’­æ›´æ–°
    4. å°†æ›´æ–°åçš„æç¤ºå‚æ•°å†™å›ç¼“å†²åŒº
    
    Args:
        local_trainer: æœ¬åœ°è®­ç»ƒå™¨
        idxs_users: å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
        local_weights: å®¢æˆ·ç«¯æƒé‡åˆ—è¡¨
        initial_weights: åˆå§‹æ¨¡å‹æƒé‡
        key_to_buffer: æç¤ºå‚æ•°åˆ°ç¼“å†²åŒºçš„æ˜ å°„
        avg_global_gradient: èšåˆåçš„å…¨å±€æ¢¯åº¦
        cluster_gradients_to_apply: åˆ†é…ç»™å„å®¢æˆ·ç«¯çš„èšç±»æ¢¯åº¦å­—å…¸
        get_prompt_default: è·å–é»˜è®¤æç¤ºå‚æ•°çš„å‡½æ•°
    """
    for idx in idxs_users:
        # ä»ç¼“å†²åŒºåŒæ­¥æç¤ºå‚æ•°åˆ° local_weights
        for key, buffer in key_to_buffer.items():
            should_handle = (
                key == 'prompt_learner.global_ctx'
                or key in local_weights[idx]
                or key in initial_weights
            )
            if not should_handle or buffer is None:
                continue
            if not isinstance(buffer[idx], torch.Tensor):
                buffer[idx] = copy.deepcopy(get_prompt_default(key, idx))
            local_weights[idx][key] = buffer[idx]
        
        # åŠ è½½æƒé‡å¹¶æ‰§è¡Œåå‘ä¼ æ’­
        local_trainer.model.load_state_dict(local_weights[idx], strict=False)
        
        cluster_grad_for_idx = None
        if cluster_gradients_to_apply is not None and idx in cluster_gradients_to_apply:
            cluster_grad_for_idx = cluster_gradients_to_apply[idx]
        
        local_trainer.train_backward(
            avg_global_gradient=avg_global_gradient,
            aggregated_cluster_gradient=cluster_grad_for_idx,
        )
        
        # å°†æ›´æ–°åçš„æç¤ºå‚æ•°å†™å›ç¼“å†²åŒº
        local_weight = local_trainer.model.state_dict()
        for key, buffer in key_to_buffer.items():
            if buffer is None or key not in local_weight:
                continue
            copied = copy.deepcopy(local_weight[key])
            buffer[idx] = copied


def format_results_table(results_list, title, client_ids):
    """
    ä½¿ç”¨ PrettyTable æ ¼å¼åŒ–æµ‹è¯•ç»“æœä¸ºè¡¨æ ¼å­—ç¬¦ä¸²ã€‚
    
    Args:
        results_list: æµ‹è¯•ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªæŒ‡æ ‡çš„å…ƒç»„
        title: è¡¨æ ¼æ ‡é¢˜
        client_ids: å®¢æˆ·ç«¯ ID åˆ—è¡¨
        
    Returns:
        str: æ ¼å¼åŒ–åçš„è¡¨æ ¼å­—ç¬¦ä¸²
    """
    if not results_list:
        return ""
    
    num_metrics = len(results_list[0])
    metric_names = ['Accuracy', 'Error Rate', 'Macro F1']
    if num_metrics > 3:
        metric_names.extend([f'Metric {i+4}' for i in range(num_metrics - 3)])
    
    table = PrettyTable()
    table.field_names = ['Client'] + metric_names[:num_metrics]
    table.align['Client'] = 'l'
    for name in metric_names[:num_metrics]:
        table.align[name] = 'r'
    
    # æ¯ä¸ªå®¢æˆ·ç«¯ä¸€è¡Œ
    for idx, res in enumerate(results_list):
        client_id = client_ids[idx] if idx < len(client_ids) else idx
        row = [f'Client {client_id}'] + [f'{val:.2f}' for val in res[:num_metrics]]
        table.add_row(row)
    
    # æœ€åä¸€è¡Œä¸ºå¹³å‡å€¼
    avg_values = []
    for i in range(num_metrics):
        avg_val = sum([res[i] for res in results_list]) / len(results_list) if results_list else 0.0
        avg_values.append(avg_val)
    avg_row = ['Average'] + [f'{val:.2f}' for val in avg_values]
    table.add_row(avg_row)
    
    table_str = f"\n{title}\n{table.get_string()}\n"
    return table_str


def run_test_phase(local_trainer, idxs_users, local_weights, key_to_buffer,
                   dirichlet, epoch, max_epoch, args, logger, wandb_run):
    """
    è¿è¡Œæµ‹è¯•é˜¶æ®µï¼Œè¯„ä¼°æ‰€æœ‰å®¢æˆ·ç«¯çš„æ¨¡å‹æ€§èƒ½ã€‚
    
    Args:
        local_trainer: æœ¬åœ°è®­ç»ƒå™¨
        idxs_users: å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
        local_weights: å®¢æˆ·ç«¯æƒé‡åˆ—è¡¨
        key_to_buffer: æç¤ºå‚æ•°åˆ°ç¼“å†²åŒºçš„æ˜ å°„
        dirichlet: æ˜¯å¦ä¸º Dirichlet åˆ’åˆ†åœºæ™¯
        epoch: å½“å‰è½®æ¬¡
        max_epoch: æœ€å¤§è½®æ¬¡
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        logger: æ—¥å¿—è®°å½•å™¨
        wandb_run: wandb è¿è¡Œå¯¹è±¡ï¼ˆå¯èƒ½ä¸º Noneï¼‰
        
    Returns:
        tuple: (avg_local_acc, avg_neighbor_acc)
            - avg_local_acc: å¹³å‡æœ¬åœ°æµ‹è¯•ç²¾åº¦
            - avg_neighbor_acc: å¹³å‡é‚»å±…æµ‹è¯•ç²¾åº¦ï¼ˆå¯èƒ½ä¸º Noneï¼‰
    """
    show_test_details = getattr(args, "test_verbose_log", False)
    test_start_time = time.time()
    
    if show_test_details:
        logger.info("")
        logger.info("=" * 80)
        logger.info(f"                    TEST START - Epoch {epoch + 1}/{max_epoch}")
        logger.info("=" * 80)
        logger.info("")
    
    local_trainer.set_model_mode("eval")
    
    results_local = []
    results_neighbor = [] if not dirichlet else None
    
    test_range = tqdm(idxs_users, desc=f"Epoch {epoch + 1}/{max_epoch} [Test]", leave=False)
    for idx in test_range:
        # ä»ç¼“å†²åŒºåŒæ­¥æç¤ºå‚æ•°
        for key, buffer in key_to_buffer.items():
            if buffer is None:
                continue
            value = buffer[idx]
            if value is None:
                continue
            if key != 'prompt_learner.global_ctx' and key not in local_weights[idx]:
                continue
            local_weights[idx][key] = value
        
        local_trainer.model.load_state_dict(local_weights[idx], strict=False)
        
        # è¯„ä¼°æœ¬åœ°æ€§èƒ½
        results_local.append(local_trainer.test(idx=idx, split='local'))
        # è¯„ä¼°é‚»å±…æ€§èƒ½ï¼ˆä»…åœ¨é Dirichlet åœºæ™¯ä½¿ç”¨ï¼‰
        if results_neighbor is not None:
            results_neighbor.append(local_trainer.test(idx=idx, split='neighbor'))
        
        # æ›´æ–°æµ‹è¯•è¿›åº¦æ¡
        local_acc = [res[0] for res in results_local]
        avg_local_acc = sum(local_acc) / len(local_acc) if local_acc else 0.0
        postfix_dict = {'local_acc': f'{avg_local_acc:.2f}%'}
        if results_neighbor is not None and len(results_neighbor) > 0:
            neighbor_acc = [res[0] for res in results_neighbor]
            avg_neighbor_acc = sum(neighbor_acc) / len(neighbor_acc) if neighbor_acc else 0.0
            postfix_dict['neighbor_acc'] = f'{avg_neighbor_acc:.2f}%'
        else:
            postfix_dict['neighbor_acc'] = 'N/A'
        test_range.set_postfix(postfix_dict)
    
    # æ ¼å¼åŒ–å¹¶è¾“å‡ºæµ‹è¯•ç»“æœ
    local_table = format_results_table(
        results_local,
        "================= Local Test Results ==================",
        idxs_users
    )
    if show_test_details:
        logger.info(local_table)
    
    local_acc = [res[0] for res in results_local]
    avg_local_acc = sum(local_acc) / len(local_acc) if local_acc else 0.0
    
    avg_neighbor_acc = None
    if results_neighbor is not None:
        neighbor_table = format_results_table(
            results_neighbor,
            "================= Neighbor Test Results ==================",
            idxs_users
        )
        if show_test_details:
            logger.info(neighbor_table)
        
        neighbor_acc = [res[0] for res in results_neighbor]
        avg_neighbor_acc = sum(neighbor_acc) / len(neighbor_acc) if neighbor_acc else 0.0
    
    test_duration = time.time() - test_start_time
    if show_test_details:
        logger.info("")
        logger.info("=" * 80)
        logger.info(f"                    TEST FINISH - Epoch {epoch + 1}/{max_epoch}")
        logger.info(f"                    æµ‹è¯•è€—æ—¶: {test_duration:.2f}s")
        logger.info("=" * 80)
        logger.info("")
    else:
        neighbor_summary = f"{avg_neighbor_acc:.2f}%" if avg_neighbor_acc is not None else "N/A"
        logger.info(
            f"[Epoch {epoch + 1}/{max_epoch}] æµ‹è¯•è€—æ—¶: {test_duration:.2f}s | "
            f"local_acc: {avg_local_acc:.2f}% | neighbor_acc: {neighbor_summary}"
        )
    
    # è®°å½•åˆ° wandb
    if wandb_run:
        log_payload = {
            "test/epoch": epoch + 1,
            "test/local_acc_avg": avg_local_acc,
            "test/duration_sec": test_duration,
        }
        if results_neighbor is not None:
            log_payload["test/neighbor_acc_avg"] = avg_neighbor_acc
        wandb.log(log_payload, step=epoch + 1)
    
    return avg_local_acc, avg_neighbor_acc


def save_training_results(args, epoch, local_weights, local_acc_list, neighbor_acc_list):
    """
    ä¿å­˜è®­ç»ƒç»“æœï¼šæ£€æŸ¥ç‚¹å’Œç²¾åº¦æ›²çº¿ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        epoch: å½“å‰è½®æ¬¡
        local_weights: å®¢æˆ·ç«¯æƒé‡åˆ—è¡¨
        local_acc_list: æœ¬åœ°ç²¾åº¦å†å²åˆ—è¡¨
        neighbor_acc_list: é‚»å±…ç²¾åº¦å†å²åˆ—è¡¨
    """
    save_checkpoint(args, epoch, local_weights, local_acc_list, neighbor_acc_list)
    output_dir = get_output_dir(args)
    os.makedirs(output_dir, exist_ok=True)
    filename_suffix = build_filename_suffix(args, prefix='acc_')
    pickle.dump(
        [local_acc_list, neighbor_acc_list],
        open(
            os.path.join(output_dir, f'{filename_suffix}.pkl'),
            'wb'
        )
    )


def log_training_summary(local_acc_list, neighbor_acc_list, dirichlet, logger, wandb_run):
    """
    è®°å½•è®­ç»ƒæ‘˜è¦ï¼šæœ€å¤§ç²¾åº¦å’Œå¹³å‡ç²¾åº¦ã€‚
    
    Args:
        local_acc_list: æœ¬åœ°ç²¾åº¦å†å²åˆ—è¡¨
        neighbor_acc_list: é‚»å±…ç²¾åº¦å†å²åˆ—è¡¨
        dirichlet: æ˜¯å¦ä¸º Dirichlet åˆ’åˆ†åœºæ™¯
        logger: æ—¥å¿—è®°å½•å™¨
        wandb_run: wandb è¿è¡Œå¯¹è±¡ï¼ˆå¯èƒ½ä¸º Noneï¼‰
    """
    # è®¡ç®—æœ¬åœ°ç²¾åº¦ç»Ÿè®¡
    if local_acc_list:
        max_local = max(local_acc_list)
        mean_local = np.mean(local_acc_list[-5:]) if len(local_acc_list) >= 5 else np.mean(local_acc_list)
        logger.info(f"maximum test local acc: {max_local:.3f}")
        logger.info(f"mean of local acc: {mean_local:.3f}")
    else:
        max_local = None
        mean_local = None
    
    # è®¡ç®—é‚»å±…ç²¾åº¦ç»Ÿè®¡
    if not dirichlet and neighbor_acc_list:
        max_neighbor = max(neighbor_acc_list)
        mean_neighbor = np.mean(neighbor_acc_list[-5:]) if len(neighbor_acc_list) >= 5 else np.mean(neighbor_acc_list)
        logger.info(f"maximum test neighbor acc: {max_neighbor:.3f}")
        logger.info(f"mean of neighbor acc: {mean_neighbor:.3f}")
    else:
        max_neighbor = None
        mean_neighbor = None
    
    # è®°å½•åˆ° wandb
    if wandb_run:
        summary_updates = {}
        if max_local is not None:
            summary_updates["summary/local_acc_max"] = max_local
        if mean_local is not None:
            summary_updates["summary/local_acc_mean"] = mean_local
        if max_neighbor is not None:
            summary_updates["summary/neighbor_acc_max"] = max_neighbor
        if mean_neighbor is not None:
            summary_updates["summary/neighbor_acc_mean"] = mean_neighbor
        wandb_run.summary.update(summary_updates)
        wandb_run.finish()




def main(args):
    """
    è”é‚¦å­¦ä¹ ä¸»å‡½æ•°ã€‚
    
    æ‰§è¡Œå®Œæ•´çš„è”é‚¦è®­ç»ƒæµç¨‹ï¼š
    1. åˆå§‹åŒ–ç¯å¢ƒï¼ˆæ—¥å¿—ã€é…ç½®ã€è®­ç»ƒå™¨ï¼‰
    2. åˆå§‹åŒ–æç¤ºå‚æ•°ç¼“å†²åŒº
    3. è®­ç»ƒå¾ªç¯ï¼ˆå¤šè½®é€šä¿¡ï¼‰
    4. æµ‹è¯•è¯„ä¼°
    5. ä¿å­˜ç»“æœå’Œæ‘˜è¦
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
    """
    # ====== åˆå§‹åŒ–æ—¥å¿—ä¸é…ç½® ======
    logger = init_logger_from_args(
        args, 
        log_dir=os.path.expanduser('~/code/sepfpl/logs'), 
        log_to_file=True, 
        log_to_console=True
    )
    
    cfg = setup_cfg(args, mode='full')
    if cfg.SEED >= 0:
        set_random_seed(cfg.SEED)
    
    if torch.cuda.is_available() and cfg.USE_CUDA:
        torch.backends.cudnn.benchmark = True
    
    # ====== åˆ¤æ–­æ˜¯å¦ä¸º Dirichlet éIID åˆ’åˆ†åœºæ™¯ ======
    # åªåœ¨ cifar10 / cifar100 ä¸­ä½¿ç”¨ neighbor è¯„ä¼°é€»è¾‘
    dirichlet = args.dataset_config_file.split('/')[-1].split('.')[0] in ['cifar10', 'cifar100']
    
    # ====== ç¡®å®šç®—æ³•ç‰¹æ€§å¼€å…³ ======
    use_hcse_flag = _use_hcse(args)
    use_time_adaptive_flag = _use_time_adaptive(args)
    
    # ====== åˆå§‹åŒ–è”é‚¦è®­ç»ƒç›¸å…³ç¼“å­˜ ======
    global_gradients = [None for _ in range(args.num_users)]
    local_weights = [{} for _ in range(args.num_users)]
    
    # ====== æ„å»ºè®­ç»ƒå™¨å¹¶ä¿å­˜åˆå§‹æƒé‡ ======
    local_trainer = build_trainer(cfg)
    wandb_run = init_wandb_run(args, cfg, logger)
    if wandb_run:
        wandb.watch(local_trainer.model, log='gradients', log_freq=200, log_graph=False)
    initial_weights = copy.deepcopy(local_trainer.model.state_dict())
    
    # ====== åˆå§‹åŒ– cluster_ctx æç¤ºï¼ˆHCSE æ¨¡å¼ï¼‰ ======
    try:
        if use_hcse_flag and 'prompt_learner.cluster_ctx' in initial_weights:
            for i in range(args.num_users):
                if 'prompt_learner.cluster_ctx' not in local_weights[i]:
                    local_weights[i]['prompt_learner.cluster_ctx'] = copy.deepcopy(
                        initial_weights['prompt_learner.cluster_ctx']
                    )
    except Exception as e:
        logger.warning(f"[Init] cluster_ctx åˆå§‹åŒ–å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼Œåç»­è®­ç»ƒä¼šè‡ªåŠ¨å†™å…¥ï¼‰ï¼š{e}")
    
    # ====== è®­ç»ƒè¿‡ç¨‹ç»Ÿè®¡é‡ ======
    start_epoch = 0
    max_epoch = cfg.OPTIM.ROUND
    local_acc_list, neighbor_acc_list = [], []
    
    # ====== ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰ ======
    if args.resume == 'True':
        start_epoch, local_weights, local_acc_list, neighbor_acc_list = load_checkpoint(args)
        logger.info(f"Resume from epoch {start_epoch}")
    if start_epoch == max_epoch - 1:
        logger.info("å·²ç»è®­ç»ƒåˆ°æœ€åä¸€è½®ï¼Œæ— éœ€ç»§ç»­")
        return
    
    # ====== ç»Ÿè®¡æ¯è½®æœ€å¤§ batch æ•° ======
    train_loaders = getattr(local_trainer, "fed_train_loader_x_dict", {})
    max_batches_per_epoch = max(len(loader) for loader in train_loaders.values()) if train_loaders else 0
    
    # ====== åˆå§‹åŒ–æç¤ºå‚æ•°ç¼“å†²åŒºæ˜ å°„ ======
    key_to_buffer = initialize_prompt_buffers(args, local_weights, use_hcse_flag)
    
    # ====== å®šä¹‰è·å–é»˜è®¤æç¤ºå‚æ•°çš„å‡½æ•° ======
    def get_prompt_default(key, idx):
        """
        åœ¨æŸäº›å®¢æˆ·ç«¯ç¼ºå¤±æŸä¸ªæç¤ºå‚æ•°æ—¶ï¼Œæä¾›ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼ã€‚
        
        Args:
            key: æç¤ºå‚æ•°é”®å
            idx: å®¢æˆ·ç«¯ç´¢å¼•
            
        Returns:
            torch.Tensor: é»˜è®¤æç¤ºå‚æ•°
        """
        if key == 'prompt_learner.global_ctx':
            return initial_weights['prompt_learner.global_ctx']
        fallback = local_weights[idx].get(
            'prompt_learner.global_ctx', 
            initial_weights['prompt_learner.global_ctx']
        )
        return initial_weights.get(key, fallback)

    # ==================== å…¨å±€é€šä¿¡è½®ä¸»å¾ªç¯ ====================
    for epoch in range(start_epoch, max_epoch):
        idxs_users = list(range(0, cfg.DATASET.USERS))
        
        # ---------- Epoch åˆå§‹åŒ–ï¼šè®¡æ—¶ä¸è®­ç»ƒç²¾åº¦ç»Ÿè®¡ ----------
        local_trainer.reset_epoch_timer()
        epoch_start_time = time.time()
        train_acc_sum = 0.0
        train_acc_count = 0
        
        # ---------- æ—¶é—´è‡ªé€‚åº”éšç§åˆ†é…ï¼šæ ¹æ®è½®æ•°æ›´æ–°å™ªå£°æ ‡å‡†å·® ----------
        if use_time_adaptive_flag and hasattr(local_trainer, 'update_std_for_round'):
            local_trainer.update_std_for_round(epoch)
        
        # ---------- è®¡ç®—å½“å‰ epoch çš„å…¨å±€å™ªå£°æ ‡å‡†å·®ï¼ˆè‹¥å¯ç”¨ DPï¼‰ ----------
        std = None
        if args.noise > 0:
            if cfg.MIA:
                std = local_trainer.std * math.sqrt(2)
            else:
                std = local_trainer.std / cfg.DATASET.USERS
        
        # ---------- ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯å‡†å¤‡æ•°æ®è¿­ä»£å™¨ ----------
        data_iters = []
        for idx in idxs_users:
            local_trainer.set_model_mode("train")
            loader = local_trainer.fed_train_loader_x_dict[idx]
            data_iters.append(iter(loader))
        
        # ---------- æ‰¹æ¬¡å¾ªç¯ï¼šæ”¶é›†æ¢¯åº¦å¹¶æ›´æ–°æç¤ºå‚æ•° ----------
        batch_range = tqdm(
            range(0, max_batches_per_epoch),
            desc=f"Epoch {epoch + 1}/{max_epoch} [Batch]",
            leave=False
        )
        
        for batch in batch_range:
            local_trainer.set_model_mode("train")
            
            # ====== æ”¶é›†å®¢æˆ·ç«¯æ¢¯åº¦ ======
            cluster_grads, batch_acc_sum, batch_acc_count = collect_client_gradients(
                local_trainer, idxs_users, data_iters, batch, max_batches_per_epoch,
                initial_weights, local_weights, global_gradients,
                key_to_buffer, use_hcse_flag, epoch
            )
            train_acc_sum += batch_acc_sum
            train_acc_count += batch_acc_count
            
            # ====== èšåˆæ¢¯åº¦ ======
            tree = None
            adj_matrix = None
            if use_hcse_flag:
                avg_global_gradient, cluster_gradients_to_apply, tree, adj_matrix = aggregate_gradients_with_hcse(
                    cluster_grads, global_gradients, idxs_users, local_trainer, args, logger
                )
            else:
                # æ—  HCSE æ—¶ï¼Œç›´æ¥å¯¹ global_gradients å–ç®€å•å¹³å‡
                avg_global_gradient = sum(global_gradients) / cfg.DATASET.USERS
                cluster_gradients_to_apply = None
            
            # ====== æ”¶é›†æ¢¯åº¦èšç±»æ•°æ®ï¼ˆæ¯ä¸ªepochæ”¶é›†ä¸€æ¬¡ï¼Œåœ¨æœ€åä¸€ä¸ªbatchæ”¶é›†ï¼‰ ======
            if cfg.GRADIENT_CLUSTERING and batch == max_batches_per_epoch - 1:
                # æ³¨æ„ï¼šè¿™é‡Œæ”¶é›†çš„æ˜¯HCSEèšåˆä¹‹å‰çš„åŸå§‹æ¢¯åº¦ï¼ˆcluster_gradsï¼‰
                # tree æ˜¯ä» aggregate_gradients_with_hcse ä¸­è·å–çš„ç¼–ç æ ‘ï¼Œç”¨äºè·å–ç¤¾åŒºID
                clustering_data = collect_gradient_clustering_data(
                    cluster_grads, idxs_users, local_trainer, tree, cfg, epoch, logger=logger
                )
                if clustering_data is not None:
                    # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
                    from pathlib import Path
                    output_dir = Path(get_output_dir(args))
                    output_dir.mkdir(parents=True, exist_ok=True)
                    filename_suffix = build_filename_suffix(args, prefix='')
                    save_path = output_dir / f'gc_e{epoch+1}_{filename_suffix}.pkl'
                    with open(save_path, 'wb') as f:
                        pickle.dump(clustering_data, f)
                    logger.info(f"âœ… æ¢¯åº¦èšç±»æ•°æ®å·²ä¿å­˜: {save_path}")
                
                # Visualize encoding tree and save as PDF
                # visualize_encoding_tree(tree, adj_matrix, output_dir, epoch, args, logger)
            
            # ====== åº”ç”¨å·®åˆ†éšç§å™ªå£° ======
            if std is not None:
                avg_global_gradient, cluster_gradients_to_apply = apply_differential_privacy_noise(
                    avg_global_gradient, cluster_gradients_to_apply, std
                )
            
            # ====== æ›´æ–°å®¢æˆ·ç«¯æƒé‡ ======
            update_client_weights(
                local_trainer, idxs_users, local_weights, initial_weights,
                key_to_buffer, avg_global_gradient, cluster_gradients_to_apply,
                get_prompt_default
            )
            
            # ====== æ›´æ–°æ‰¹æ¬¡è¿›åº¦æ¡ä¿¡æ¯ ======
            if train_acc_count > 0:
                avg_acc = train_acc_sum / train_acc_count
                batch_range.set_postfix({'avg_acc': f'{avg_acc:.2f}%'})
            else:
                batch_range.set_postfix({'avg_acc': 'N/A'})
        
        # ====== æ¯è½®è®­ç»ƒç»“æŸåçš„æ—¥å¿—ä¸ wandb è®°å½• ======
        train_stage_duration = time.time() - epoch_start_time
        avg_train_acc = (train_acc_sum / train_acc_count) if train_acc_count > 0 else 0.0
        logger.info(
            f"[Epoch {epoch + 1}/{max_epoch}] è®­ç»ƒè€—æ—¶: {train_stage_duration:.2f}s | "
            f"å¹³å‡è®­ç»ƒå‡†ç¡®ç‡: {avg_train_acc:.2f}"
        )
        if wandb_run:
            wandb.log(
                {
                    "train/epoch": epoch + 1,
                    "train/avg_acc": avg_train_acc,
                    "train/duration_sec": train_stage_duration,
                },
                step=epoch + 1,
            )
        # ==================== æµ‹è¯•é˜¶æ®µ ====================
        # æµ‹è¯•ç­–ç•¥ï¼šå‰2ä¸ªepochè®­ç»ƒå®Œåæ‰§è¡Œç¬¬ä¸€æ¬¡æµ‹è¯•ï¼Œä¹‹ååœ¨å¥‡æ•°epochæ—¶æµ‹è¯•ï¼Œæˆ–è€…æœ€åä¸€è½®ä¹Ÿæµ‹è¯•
        should_test = not getattr(args, 'skip_test', False) and ((epoch % 2 == 1) or (epoch == max_epoch - 1))
        
        if should_test:
            avg_local_acc, avg_neighbor_acc = run_test_phase(
                local_trainer, idxs_users, local_weights, key_to_buffer,
                dirichlet, epoch, max_epoch, args, logger, wandb_run
            )
            local_acc_list.append(avg_local_acc)
            if avg_neighbor_acc is not None:
                neighbor_acc_list.append(avg_neighbor_acc)
        
        # ---------- ç¡®ä¿ local_weights åŒ…å«æœ€æ–°çš„æƒé‡ï¼ˆä» buffer åŒæ­¥ï¼‰ ----------
        for idx in idxs_users:
            for key, buffer in key_to_buffer.items():
                if buffer is None or buffer[idx] is None:
                    continue
                local_weights[idx][key] = copy.deepcopy(buffer[idx])
        
        # ---------- ä¿å­˜æ£€æŸ¥ç‚¹ & ç²¾åº¦æ›²çº¿ ----------
        save_training_results(args, epoch, local_weights, local_acc_list, neighbor_acc_list)
    
    # ==================== è®­ç»ƒç»“æŸï¼Œè®°å½• summary ====================
    log_training_summary(local_acc_list, neighbor_acc_list, dirichlet, logger, wandb_run)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    # ====== é€šä¿¡ä¸è®­ç»ƒåŸºæœ¬å‚æ•° ======
    parser.add_argument('--round', type=int, default=20,
                        help="å…¨å±€é€šä¿¡è½®æ•°ï¼ˆfederated roundsï¼‰")
    parser.add_argument('--num-users', type=int, default=10,
                        help="å®¢æˆ·ç«¯æ•°é‡")
    parser.add_argument('--lr', type=float, default=0.001,
                        help='å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰')
    parser.add_argument('--train-batch-size', type=int, default=32,
                        help="è®­ç»ƒ batch sizeï¼ˆä»…åœ¨ useall=True æ—¶ç”Ÿæ•ˆï¼‰")
    parser.add_argument('--test-batch-size', type=int, default=100,
                        help="æµ‹è¯• batch size")
    parser.add_argument("--seed", type=int, default=1,
                        help="éšæœºç§å­ï¼ˆ>0 å¯ç”¨å›ºå®šéšæœºæ€§ï¼‰")

    # ====== çŸ©é˜µåˆ†è§£ & å·®åˆ†éšç§ç›¸å…³å‚æ•° ======
    parser.add_argument(
        '--factorization', type=str, default='dpfpl',
        help='çŸ©é˜µåˆ†è§£ / è”é‚¦æç¤ºå­¦ä¹ ç®—æ³•ï¼š'
             'promptfl, fedotp, fedpgp, dplora, dpfpl, sepfpl, sepfpl_time_adaptive, sepfpl_hcse'
    )
    parser.add_argument('--rank', type=int, default=8,
                        help='çŸ©é˜µåˆ†è§£çš„ç§©ï¼ˆrankï¼‰')
    parser.add_argument('--norm-thresh', type=float, default=10.0,
                        help='æ¢¯åº¦è£å‰ªçš„èŒƒæ•°é˜ˆå€¼ï¼ˆclipping normï¼‰')
    parser.add_argument('--noise', type=float, default=0.0,
                        help='å·®åˆ†éšç§é«˜æ–¯å™ªå£°å°ºåº¦ï¼ˆæ ‡å‡†å·®ï¼‰')
    parser.add_argument('--rdp-alpha', type=float, default=2.0,
                        help='RDPï¼ˆRÃ©nyi DPï¼‰é˜¶æ•° Î±')
    parser.add_argument('--rdp-p', type=float, default=0.2,
                        help='sepfpl ä¸­æ—¶é—´è‡ªé€‚åº”éšç§é¢„ç®—åˆ†é…çš„å¹‚æ¬¡ p')

    # ====== æ•°æ®é›†ç›¸å…³å‚æ•° ======
    # caltech101, oxford_flowers, oxford_pets, food101, dtd ç­‰
    parser.add_argument(
        '--iid', default=False,
        help="æ˜¯å¦å¯¹ä¸Šè¿°æ•°æ®é›†é‡‡ç”¨ IID åˆ’åˆ†ï¼ˆTrueï¼šIIDï¼›Falseï¼šé IIDï¼‰"
    )
    parser.add_argument(
        '--num-shots', type=int, default=16,
        help="few-shot è®¾ç½®ä¸‹çš„æ¯ç±»æ ·æœ¬æ•°ï¼ˆä»…åœ¨ useall=False æ—¶ç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        '--useall', default=True,
        help="æ˜¯å¦ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ ·æœ¬ï¼ˆTrueï¼šå…¨é‡è®­ç»ƒï¼›Falseï¼šfew-shotï¼‰"
    )
    # cifar10, cifar100 ç­‰
    parser.add_argument(
        '--partition', type=str, default='noniid-coarsegroup-iid',
        help='cifar10/cifar100 çš„æ•°æ®åˆ’åˆ†ç­–ç•¥ï¼š'
             '"homo, noniid-labeluni, noniid-labeldir, noniid-labeldir100, noniid-coarsegroup-iid"'
    )
    parser.add_argument(
        '--beta', type=float, default=0.3,
        help='Dirichlet åˆ†å¸ƒå‚æ•° Î²ï¼Œç”¨äºé IID æ•°æ®åˆ’åˆ†'
    )

    # ====== å¯å­¦ä¹ æç¤ºï¼ˆpromptï¼‰ç›¸å…³å‚æ•° ======
    parser.add_argument(
        '--n_ctx', type=int, default=16,
        help="æ–‡æœ¬æç¤ºçš„ context token æ•°é‡"
    )
    parser.add_argument(
        '--sepfpl-topk', type=int, default=8,
        help='HCSE ä¸­æ„å»ºç›¸ä¼¼åº¦å›¾æ—¶çš„ top-k é‚»å±…æ•°ï¼ˆä»… sepfpl ç›¸å…³æ–¹æ³•ä½¿ç”¨ï¼‰'
    )

    # ====== è·¯å¾„ç›¸å…³å‚æ•° ======
    parser.add_argument(
        "--root", type=str, default="/datasets",
        help="æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--config-file", type=str,
        default="configs/trainers/DP-FPL/vit_b16.yaml",
        help="è®­ç»ƒå™¨ / æ¨¡å‹çš„é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--dataset-config-file", type=str,
        default="configs/datasets/cifar100.yaml",
        help="æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--resume", type=str, default="False",
        help="æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆ'True'/'False'ï¼‰"
    )
    parser.add_argument(
        '--task-id', type=str, default=None,
        help='ä»»åŠ¡ç¼–å·æ ‡è¯†ï¼Œæ ¼å¼å»ºè®®å¦‚ "[1/100]"ï¼ˆç”¨äºæ—¥å¿—ä¸ wandb æ ‡è®°ï¼‰'
    )
    # å¯é€‰ CLIP æ¨¡å‹è·¯å¾„
    parser.add_argument(
        '--clip-model-path', type=str, default=None,
        help='æœ¬åœ° CLIP æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰'
    )
    parser.add_argument(
        '--clip-cache-dir', type=str, default=None,
        help='CLIP æ¨¡å‹ä¸‹è½½ / ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰'
    )

    # ====== wandb æ—¥å¿—é…ç½® ======
    parser.add_argument(
        '--wandb-group', type=str, default=None,
        help='wandb ä¸­çš„å®éªŒåˆ†ç»„åï¼ˆgroupï¼‰ï¼Œç”¨äºç»„ç»‡ä¸€ç»„ç›¸å…³å®éªŒ'
    )

    # ====== æµ‹è¯•æ§åˆ¶å‚æ•° ======
    parser.add_argument(
        '--skip-test', action='store_true', default=False,
        help='è·³è¿‡æµ‹è¯•é˜¶æ®µï¼ˆé»˜è®¤è¿›è¡Œæµ‹è¯•ï¼‰'
    )

    args = parser.parse_args()
    main(args)
